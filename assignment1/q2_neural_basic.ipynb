{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q2 sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# q2_sigmoid\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    x = 1. / (1 + np.exp(-x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_grad(f):\n",
    "    f = f * (1 - f)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q1 softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax function for each row of the input x\n",
    "    \n",
    "    It is crucial that this function is optimized for speed because\n",
    "    it will be used frequently in later code.\n",
    "    You might find numpy functions np.exp, np.sum, np.reshape, np.max,\n",
    "    and numpy broadcasting usefull for this task\n",
    "    \n",
    "    You should also make sure that your code works for one dimensional inputs\n",
    "    \n",
    "    \"\"\"\n",
    "    if len(x.shape) > 1:\n",
    "        tmp = np.max(x, axis = 1)\n",
    "        x -= tmp.reshape((x.shape[0], 1))\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x, axis = 1)\n",
    "        x /= tmp.reshape((x.shape[0], 1))\n",
    "    else:\n",
    "        tmp = np.max(x)\n",
    "        x -= tmp\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x)\n",
    "        x /= tmp\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q2 gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "def gradcheck_naive(f, x):\n",
    "    \"\"\"\n",
    "    Gradient check for a function f\n",
    "    -f should be a function that takes a single argument and outputs the cost\n",
    "    and its gradients\n",
    "    -x is the point (numpy array) to check the gradient at\"\"\"\n",
    "    \n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)\n",
    "    fx, grad = f(x)\n",
    "    h = 1e-4\n",
    "    \n",
    "    # Iterate over all indexed in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        \n",
    "        ### try modifying x[ix] with h defined above to compute numerical gradients\n",
    "        ### make sure you call random.setstate(rndstate) before calling f(x) each time, this will make it\n",
    "        ### possible to test cost functions with built in randomness later\n",
    "        ### \n",
    "        old_xix = x[ix]\n",
    "        x[ix] = old_xix + h\n",
    "        random.setstate(rndstate)\n",
    "        fp = f(x)[0]\n",
    "        x[ix] = old_xix - h\n",
    "        random.setstate(rndstate)\n",
    "        fm = f(x)[0]\n",
    "        x[ix] = old_xix\n",
    "        \n",
    "        numgrad = (fp - fm)/(2 * h)\n",
    "        \n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print(\"Gradient check failed.\")\n",
    "            print(\"First gradient error found at index %s\" % str(ix))\n",
    "            print(\"Your gradient: %f \\t Numerical gradient: %f\" %(grad[ix], numgrad))\n",
    "            \n",
    "            return\n",
    "    it.iternext()\n",
    "print(\"Gradient check passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_numerical(f, x, h=1e-4):\n",
    "    \"\"\"\n",
    "    Gradient check for a function f\n",
    "    -f: should be a function that takes a single argument and outputs the cost\n",
    "    and its gradients\n",
    "    -x: is the point (numpy array) to check the gradient at \n",
    "    -h: is the size of the shift for all dimensions\"\"\"\n",
    "    \n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)\n",
    "    fx, grad = f(x)\n",
    "    num_grad = np.zeros(x.shape)\n",
    "    \n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        \n",
    "        ### try modifying x[ix] with h defined above to compute numerical gradient\n",
    "        ### make sure you callrandom.setstate(rndstate) before calling f(x) each time, this will make it\n",
    "        ### possible to test cost functions with built in randomness later\n",
    "        old_xix = x[ix]\n",
    "        x[ix] += 0.5 * h\n",
    "        random.setstate(rndstate)\n",
    "        fp = f(x)[0]\n",
    "        x[ix] -= h\n",
    "        random.setstate(rndstate)\n",
    "        fm = f(x)[0]\n",
    "        x[ix] = old_xix\n",
    "        \n",
    "        num_grad += (fp - fm)/h\n",
    "        it.iternext()\n",
    "    return num_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_numerical_gradient_array(f, x, df, h = 1e-5):\n",
    "    # Evaluate a numeric for a function that accepts a numpy array and returns an numpy array\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        \n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        pos = f(x).copy()\n",
    "        x[ix] = oldval - h\n",
    "        neg = f(x).copy()\n",
    "        x[ix] = oldval\n",
    "        \n",
    "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q2 neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computer the forward pass for an affine (fully-connected) layer\n",
    "    The input x has shape (N, d_1, , ... d_k) and contains a\n",
    "    minibatch of N examples, where each example x[i] has shape (d_1, ..., d_k).\n",
    "    We will reshape each input into a vector of dimension D = d_1 * ... * d_k and\n",
    "    then transform it to an output vector of dimension M.\n",
    "    \n",
    "    Inputs:\n",
    "    x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    w: a numpy array of weights, of shape (D, M)\n",
    "    b: a numpy array of biases, of shape (M, )\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    out: output, of shape (N, M)\n",
    "    cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    N = x.shape[0]\n",
    "    D = np.prod(x.shape[1:])\n",
    "    M = b.shape[1]\n",
    "    out = np.dot(x.reshape(N, D), w.reshape(D, M)) + b.reshape(1, M)\n",
    "    \n",
    "    return out, (x, w, b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "b = np.array([[8, 8, 8]])\n",
    "out = a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9 10 11]\n",
      " [12 13 14]]\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer\n",
    "    \n",
    "    Inputs:\n",
    "    -dout: Upstream derivative, of shape (N, M)\n",
    "    -cache: Tuple of:\n",
    "    -x: Input data, of shape (N, d_1, ..., d_k)\n",
    "    -w: Weights, of shape (D, M)\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    -dx: Gradient with respect to x, of shape (N, d1, ... d_k)\n",
    "    -dw: Gradient with respect to w, of shape (D, M)\n",
    "    -db: Gradient with respect to b, of shape (M,)\"\"\"\n",
    "    \n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "    N = x.shape[0]\n",
    "    D = np.prod(x.shape[1:])\n",
    "    M = b.shape[1]\n",
    "    \n",
    "    dx = np.dot(dout, w.reshape(D, M).T).reshape(x.shape) #DxN\n",
    "    dw = np.dot(x.reshape(N, D).T, dout).reshape(w.shape) #MxD\n",
    "    db = np.sum(dout, axis=0) #1xM\n",
    "    \n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a sigmoid activation.\n",
    "    Inputs:\n",
    "    -x: Input data, numpy array of arbitary shape\n",
    "    \n",
    "    Returns a tuple (out, cache)\n",
    "    -out: output of the same shape as x\n",
    "    -cache: identical to out; required for backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    return sigmoid(x), sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an sigmoid layer.\n",
    "    \n",
    "    Inputs:\n",
    "    -dout: Upstream derivative, same shape as the input to the \n",
    "    sigmoid layer (x)\n",
    "    -cache: sigmoid(x)\n",
    "    Returns a tuple of:\n",
    "    -dx: back propagated gradient with respect to x\n",
    "    \"\"\"\n",
    "    \n",
    "    x = cache\n",
    "    return sigmoid_grad(x) * dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_backward_prop(data, labels, params, dimensions):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation for a two-layer sigmoidal network\n",
    "    \n",
    "    Compute the forward propagation and for the cross entropy cost, and backward propagation for the gradients for all parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    ofs = 0\n",
    "    Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2])\n",
    "    N = data.shape[0]\n",
    "    \n",
    "    W1 = np.reshape(params[ofs:ofs + Dx * H], (Dx, H))\n",
    "    ofs += Dx * H\n",
    "    b1 = np.reshape(params[ofs:ofs + H], (1, H))\n",
    "    ofs += H\n",
    "    W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))\n",
    "    ofs += H * Dy\n",
    "    b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy))\n",
    "    \n",
    "    # forward propagation\n",
    "    hidden = np.dot(data, W1) + b1\n",
    "    layer1_a = sigmoid(hidden)\n",
    "    layer2 = np.dot(layer1_a, W2) + b2\n",
    "    \n",
    "    # need to calculate the softmax loss\n",
    "    probs = softmax(layer2)\n",
    "    costs = -np.sum(np.log(probs[np.arange(N), np.argmax(labels, axis=1)]))\n",
    "    \n",
    "    # backward propagation\n",
    "    # There is no regularization\n",
    "    # dx -> sigmoid -> w2 * layer1_a+ b -> sigmoid -> W1 * data + b1 ->\n",
    "    dx = probs.copy()\n",
    "    dx -= labels\n",
    "    \n",
    "    dlayer2 = np.zeros_like(dx)\n",
    "    gradW2 = np.zeros_like(W2)\n",
    "    gradW1 = np.zeros_like(W1)\n",
    "    gradb2 = np.zeros_like(b2)\n",
    "    gradb1 = np.zeros_like(b1)\n",
    "    \n",
    "    gradw2 = np.dot(layer1_a.T, dx)\n",
    "    gradb2 = np.sum(dx, axis=0)\n",
    "    dlayer2 = np.dot(dw, W2.T)\n",
    "    dlayer1 = sigmoid_grad(layer1_a) * dlayer2\n",
    "    gradW1 = np.dot(data.T, dlayer1)\n",
    "    gradb1 = np.sum(dlayer1, axis = 0)\n",
    "    \n",
    "    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(), gradW2.flatten(), gradb2.flatten()))\n",
    "    \n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sanity_check():\n",
    "    \"\"\"\n",
    "    Set up fake data and parameters for the neural network, and test using\n",
    "    gradcheck.\"\"\"\n",
    "    print(\"Running sanity check...\")\n",
    "    N = 300\n",
    "    dimensions = [10, 5, 10]\n",
    "    data = np.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
