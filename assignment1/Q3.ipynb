{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from q1_softmax import softmax\n",
    "from q2_gradcheck import gradcheck_naive\n",
    "from q2_sigmoid import sigmoid, sigmoid_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeRows(x):\n",
    "    \"\"\"\n",
    "    Row normalization function\n",
    "    Implement a function that normalizes each row of a matrix to have unit length\n",
    "    \"\"\"\n",
    "    denom = np.apply_along_axis(lambda x: np.sqrt(x.T.dot(x)), 1, x)\n",
    "    x /= denom[:, None]\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test normalize rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6         0.8       ]\n",
      " [ 0.4472136   0.89442719]]\n"
     ]
    }
   ],
   "source": [
    "x = normalizeRows(np.array([[3.0, 4.0], [1, 2]]))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### np.apply_along_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(a):\n",
    "    \"\"\"Average first and last element of a 1-D array\"\"\"\n",
    "    return (a[0] + a[-1]) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.,  5.,  6.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "np.apply_along_axis(my_func, 0, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  5.,  8.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.apply_along_axis(my_func, 1, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax cost function for word2vec models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxCostAndGradient(predicted, target, outputVectors, dataset):\n",
    "    \"\"\"\n",
    "    Implement the cost and gradients for one predicted word vector and one target vector as a building block for word2vec models,\n",
    "    assuming the softmax prediction function and cross entropy loss.\"\"\"\n",
    "    \n",
    "   #  Calculate the predictions:\n",
    "    vhat = predicted\n",
    "    z = np.dot(outputVectors, vhat)\n",
    "    preds = softmax(z)\n",
    "\n",
    "    #  Calculate the cost:\n",
    "    cost = -np.log(preds[target])\n",
    "\n",
    "    #  Gradients\n",
    "    z = preds.copy()\n",
    "    z[target] -= 1.0\n",
    "\n",
    "    grad = np.outer(z, vhat)\n",
    "    gradPred = np.dot(outputVectors.T, z)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradPred, grad\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.6596429545646822"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(2121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[None] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNegativeSamples(target, dataset, K):\n",
    "    \"\"\"\n",
    "    Samples K indexes which are not the target\n",
    "    \"\"\"\n",
    "    indices = [None] * K\n",
    "    for k in range(K):\n",
    "        newdix = dataset.sampleTokenIdx()\n",
    "        while newidx == target:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        indices[k] = newidx\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative sampling cost function for word2vec models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negSamplingCostAndGradient(predicted, target, outputVectors, dataset, K=10):\n",
    "    indices = [target]\n",
    "    indices.extend(getNegativeSamples(target, dataset, K))\n",
    "    \n",
    "    grad = np.zeros(outputVectors.shape)\n",
    "    gradPred = np.zeros(predicted.shape)\n",
    "    cost = 0\n",
    "    z = sigmoid(np.dot(outputVectors[target], predicted))\n",
    "    \n",
    "    cost -= np.log(z)\n",
    "    grad[target] += predicted * (z - 1.0)\n",
    "    gradPred += outputVectors[target] * (z - 1.0)\n",
    "    \n",
    "    for k in range(K):\n",
    "        samp = indices[k + 1]\n",
    "        z = sigmoid(np.dot(outputVectors[samp], predicted))\n",
    "        \n",
    "        cost -= np.log(Z)\n",
    "        grad[target] += predicted * (z - 1.0)\n",
    "        gradPred += outputVectors[target] * (z - 1.0)\n",
    "        \n",
    "        for k in range(K):\n",
    "            samp = indices[k + 1]\n",
    "            z = sigmoid(np.dot(outputVectors[samp], predicted))\n",
    "            cost -= np.log(1.0 - z)\n",
    "            grad[samp] += predicted * z\n",
    "            gradPred += outputVectors[samp] * z\n",
    "    \n",
    "    return cost, gradPred, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip gram model in word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors, dataset, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    \"\"\"\n",
    "    Implement the skip-gram model in this function\n",
    "    \n",
    "    Arguments:\n",
    "        CurrentWord -- a string of the current center word\n",
    "        C -- integer, context size\n",
    "        contextWords -- list of no more than 2*C strings, the context words\n",
    "        tokens -- a dictionary that maps words to their indices in the word vector list\n",
    "\n",
    "        tokens -- a dictionary that maps words to their indices in the word vector list\n",
    "\n",
    "        inputVectors -- input word vectors (as rows) for all tokens\n",
    "        outputVectors -- output word vectors (as rows) for all tokens\n",
    "        word2vecCostAndGradient -- the cost and gradient function for a predictino vector given the target word vectors, could be one of the two\n",
    "        cost function you implemented above.\n",
    "\n",
    "    return \n",
    "        cost -- the cost function value for the skip-gram model\n",
    "        grad -- the gradient with respect to the word vectores\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    cost = 0.0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "    \n",
    "    cword_idx = tokens[currentWord]\n",
    "    vhat = inputVectors[cword_idx]\n",
    "    \n",
    "    for j in contextWords:\n",
    "        u_idx = tokens[j]\n",
    "        c_cost, c_grad_in, c_grad_out = word2vecCostAndGradient(vhat, u_idx, outputVectors, dataset)\n",
    "        cost += c_cost\n",
    "        gradIn[cword_idx] += c_grad_in\n",
    "        gradOut += c_grad_out\n",
    "        \n",
    "    return cost, gradIn, gradOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW model in word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow(currentWord, C, contextWords, tokens, inputVectors, outputVectors, dataset, word2vecCostAndGradient=softmaxCostAndGradient):\n",
    "    \"\"\"CBOW model in word2vec\n",
    "    \n",
    "    Implement the continuous bag-of-words model in this function.\n",
    "    \n",
    "    Arguments/Return specifications: same as the skip-gram model\n",
    "    \n",
    "    Extra credit: Implementing CBOW is optional, but the gradient derivations are not. If you decide not to implement CBOW\n",
    "    , remove the NotImplementedError\n",
    "    \"\"\"\n",
    "    cost = 0.0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "   \n",
    "    predicted_indices = [tokens[word] for word in contextWords]\n",
    "    predicted_vectors = inputVectors[predicted_indices]\n",
    "    \n",
    "    predicted = np.sum(predicted_vectors, axis=0)\n",
    "    target = tokens[currentWord]\n",
    "    \n",
    "    cost, gradIn_predicted, gradOut = word2vecCostAndGradient(predicted, target, outputVectors, dataset)\n",
    "    for i in predicted_indices:\n",
    "        gradIn[i] += gradIn_predicted\n",
    "        \n",
    "    return cost, gradIn, gradOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing functions below. Do not MODIFY!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_sgd_wrapper(word2vecModel, tokens, wordVectors, dataset, C, word2vecCostAndGradient=softmaxCostAndGradient):\n",
    "    batchsize = 50\n",
    "    cost = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    inputVectors = wordVectors[:(N / 2), :]\n",
    "    outputVectors = wordVectors[(N / 2):, :]\n",
    "    \n",
    "    for i in range(batchsize):\n",
    "        C1 = random.randint(1, C)\n",
    "        centerword, context = dataset.getRandomContext(C1)\n",
    "        \n",
    "        if word2vecModel == skipgram:\n",
    "            denom = 1\n",
    "        else:\n",
    "            denom = 1\n",
    "            \n",
    "        c, gin, gout = word2vecModel(centerword, C1, context, tokens, inputVectors, outputVectors, dataset, word2vecCostAndGradient)\n",
    "        \n",
    "        cost += c / batchsize / denom\n",
    "        grad[:N / 2, :] += gin / batchsize / denom\n",
    "        grad[N / 2:, :] += gout / batchsize /denom\n",
    "    \n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interface to the dataset for negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_word2vec():\n",
    "    dataset = type('dummy', (), {})()\n",
    "    \n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "    \n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0, 4)], \\\n",
    "                [tokens[random.randint(0, 4)] for i in range(2 * C)]\n",
    "        \n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "    \n",
    "    random.seed(32123)\n",
    "    np.random.seed(3883)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10, 3))\n",
    "    dummy_tokens = dict([(\"a\", 0), (\"b\", 1), (\"c\", 2), (\"d\", 3), (\"e\", 4)])\n",
    "    \n",
    "    print(\"===Gradient check for skip-gram===\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient), dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient), dummy_vectors)\n",
    "    print(\"\\n==== Gradient check for CBOW ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(cbow, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient), dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(cbow, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient), dummy_vectors)\n",
    "    \n",
    "    print(\"\\n===Results===\")\n",
    "    print(skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"], dummy_tokens, dummy_vectors[:5, :], dummy_vectors[5:, :], dataset))\n",
    "    print(skipgram(\"c\", 1, [\"a\", \"b\"], dummy_tokens, dummy_vectors[:5, :], dummy_vectors[5:, :], dataset, negSamplingCostAndGradient))\n",
    "    print(cbow(\"a\", 2, [\"a\", \"b\", \"c\", \"a\"], dummy_tokens, dummy_vectors[:5, :], dummy_vectors[5:, :], dataset))\n",
    "    print(cbow(\"a\", 2, [\"a\", \"b\", \"a\", \"c\"], dummy_tokens, dummy_vectors[:5, :], dummy_vectors[5:, :], dataset, negSamplingCostAndGradient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Gradient check for skip-gram===\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-26f03a9b22fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-872f29f6ec32>\u001b[0m in \u001b[0;36mtest_word2vec\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"===Gradient check for skip-gram===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mgradcheck_naive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword2vec_sgd_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskipgram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmaxCostAndGradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mgradcheck_naive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword2vec_sgd_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskipgram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegSamplingCostAndGradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n==== Gradient check for CBOW ====\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/damvantai/Dropbox/source/courses/CS224N-2017 Deep Learning for Natural Language Processing/assignment1/q2_gradcheck.py\u001b[0m in \u001b[0;36mgradcheck_naive\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mrndstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrndstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mfx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-872f29f6ec32>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(vec)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"===Gradient check for skip-gram===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mgradcheck_naive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword2vec_sgd_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskipgram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmaxCostAndGradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mgradcheck_naive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword2vec_sgd_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskipgram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegSamplingCostAndGradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n==== Gradient check for CBOW ====\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-9b597dc2ee2e>\u001b[0m in \u001b[0;36mword2vec_sgd_wrapper\u001b[0;34m(word2vecModel, tokens, wordVectors, dataset, C, word2vecCostAndGradient)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vecModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenterword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vecCostAndGradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatchsize\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-381b60fb34ef>\u001b[0m in \u001b[0;36mskipgram\u001b[0;34m(currentWord, C, contextWords, tokens, inputVectors, outputVectors, dataset, word2vecCostAndGradient)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontextWords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mu_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mc_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_grad_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_grad_out\u001b[0m \u001b[0;34m=\u001b[0m             \u001b[0mword2vecCostAndGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc_cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mgradIn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcword_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc_grad_in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-381b60fb34ef>\u001b[0m in \u001b[0;36msoftmaxCostAndGradient\u001b[0;34m(predicted, target, outputVectors, dataset)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m#  Calculate the cost:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m#  Gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "test_word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing normalizeRows...\n",
      "[[ 0.6         0.8       ]\n",
      " [ 0.4472136   0.89442719]]\n",
      "\n",
      "==== Gradient check for skip-gram ====\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-843f480616bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0mtest_normalize_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m     \u001b[0mtest_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-843f480616bd>\u001b[0m in \u001b[0;36mtest_word2vec\u001b[0;34m()\u001b[0m\n\u001b[1;32m    259\u001b[0m     gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n\u001b[1;32m    260\u001b[0m         skipgram, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n\u001b[0;32m--> 261\u001b[0;31m                     dummy_vectors)\n\u001b[0m\u001b[1;32m    262\u001b[0m     gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n\u001b[1;32m    263\u001b[0m         skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient),\n",
      "\u001b[0;32m/home/damvantai/Dropbox/source/courses/CS224N-2017 Deep Learning for Natural Language Processing/assignment1/q2_gradcheck.py\u001b[0m in \u001b[0;36mgradcheck_naive\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mrndstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrndstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mfx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-843f480616bd>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(vec)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"==== Gradient check for skip-gram ====\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n\u001b[0;32m--> 260\u001b[0;31m         skipgram, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n\u001b[0m\u001b[1;32m    261\u001b[0m                     dummy_vectors)\n\u001b[1;32m    262\u001b[0m     gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
      "\u001b[0;32m<ipython-input-24-843f480616bd>\u001b[0m in \u001b[0;36mword2vec_sgd_wrapper\u001b[0;34m(word2vecModel, tokens, wordVectors, dataset, C, word2vecCostAndGradient)\u001b[0m\n\u001b[1;32m    230\u001b[0m         c, gin, gout = word2vecModel(\n\u001b[1;32m    231\u001b[0m             \u001b[0mcenterword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputVectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             dataset, word2vecCostAndGradient)\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatchsize\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgin\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatchsize\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-843f480616bd>\u001b[0m in \u001b[0;36mskipgram\u001b[0;34m(currentWord, C, contextWords, tokens, inputVectors, outputVectors, dataset, word2vecCostAndGradient)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontextWords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mu_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mc_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_grad_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_grad_out\u001b[0m \u001b[0;34m=\u001b[0m             \u001b[0mword2vecCostAndGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc_cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mgradIn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcword_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc_grad_in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-843f480616bd>\u001b[0m in \u001b[0;36msoftmaxCostAndGradient\u001b[0;34m(predicted, target, outputVectors, dataset)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m#  Calculate the cost:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m#  Gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from q1_softmax import softmax\n",
    "from q2_gradcheck import gradcheck_naive\n",
    "from q2_sigmoid import sigmoid, sigmoid_grad\n",
    "\n",
    "\n",
    "def normalizeRows(x):\n",
    "    \"\"\" Row normalization function\n",
    "\n",
    "    Implement a function that normalizes each row of a matrix to have\n",
    "    unit length.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    denom = np.apply_along_axis(lambda x: np.sqrt(x.T.dot(x)), 1, x)\n",
    "    x /= denom[:, None]\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def test_normalize_rows():\n",
    "    print \"Testing normalizeRows...\"\n",
    "    x = normalizeRows(np.array([[3.0, 4.0], [1, 2]]))\n",
    "    print x\n",
    "    ans = np.array([[0.6, 0.8], [0.4472136, 0.89442719]])\n",
    "    assert np.allclose(x, ans, rtol=1e-05, atol=1e-06)\n",
    "    print \"\"\n",
    "\n",
    "\n",
    "def softmaxCostAndGradient(predicted, target, outputVectors, dataset):\n",
    "    \"\"\" Softmax cost function for word2vec models\n",
    "\n",
    "    Implement the cost and gradients for one predicted word vector\n",
    "    and one target word vector as a building block for word2vec\n",
    "    models, assuming the softmax prediction function and cross\n",
    "    entropy loss.\n",
    "\n",
    "    Arguments:\n",
    "    predicted -- numpy ndarray, predicted word vector (\\hat{v} in\n",
    "                 the written component)\n",
    "    target -- integer, the index of the target word\n",
    "    outputVectors -- \"output\" vectors (as rows) for all tokens\n",
    "    dataset -- needed for negative sampling, unused here.\n",
    "\n",
    "    Return:\n",
    "    cost -- cross entropy cost for the softmax word prediction\n",
    "    gradPred -- the gradient with respect to the predicted word\n",
    "           vector\n",
    "    grad -- the gradient with respect to all the other word\n",
    "           vectors\n",
    "\n",
    "    We will not provide starter code for this function, but feel\n",
    "    free to reference the code you previously wrote for this\n",
    "    assignment!\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    ## Gradient for $\\hat{\\bm{v}}$:\n",
    "\n",
    "    #  Calculate the predictions:\n",
    "    vhat = predicted\n",
    "    z = np.dot(outputVectors, vhat)\n",
    "    preds = softmax(z)\n",
    "\n",
    "    #  Calculate the cost:\n",
    "    cost = -np.log(preds[target])\n",
    "\n",
    "    #  Gradients\n",
    "    z = preds.copy()\n",
    "    z[target] -= 1.0\n",
    "\n",
    "    grad = np.outer(z, vhat)\n",
    "    gradPred = np.dot(outputVectors.T, z)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradPred, grad\n",
    "\n",
    "\n",
    "def getNegativeSamples(target, dataset, K):\n",
    "    \"\"\" Samples K indexes which are not the target \"\"\"\n",
    "\n",
    "    indices = [None] * K\n",
    "    for k in xrange(K):\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "        while newidx == target:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        indices[k] = newidx\n",
    "    return indices\n",
    "\n",
    "\n",
    "def negSamplingCostAndGradient(predicted, target, outputVectors, dataset,\n",
    "                               K=10):\n",
    "    \"\"\" Negative sampling cost function for word2vec models\n",
    "\n",
    "    Implement the cost and gradients for one predicted word vector\n",
    "    and one target word vector as a building block for word2vec\n",
    "    models, using the negative sampling technique. K is the sample\n",
    "    size.\n",
    "\n",
    "    Note: See test_word2vec below for dataset's initialization.\n",
    "\n",
    "    Arguments/Return Specifications: same as softmaxCostAndGradient\n",
    "    \"\"\"\n",
    "\n",
    "    # Sampling of indices is done for you. Do not modify this if you\n",
    "    # wish to match the autograder and receive points!\n",
    "    indices = [target]\n",
    "    indices.extend(getNegativeSamples(target, dataset, K))\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    grad = np.zeros(outputVectors.shape)\n",
    "    gradPred = np.zeros(predicted.shape)\n",
    "    cost = 0\n",
    "    z = sigmoid(np.dot(outputVectors[target], predicted))\n",
    "\n",
    "    cost -= np.log(z)\n",
    "    grad[target] += predicted * (z - 1.0)\n",
    "    gradPred += outputVectors[target] * (z - 1.0)\n",
    "\n",
    "    for k in xrange(K):\n",
    "        samp = indices[k + 1]\n",
    "        z = sigmoid(np.dot(outputVectors[samp], predicted))\n",
    "        cost -= np.log(1.0 - z)\n",
    "        grad[samp] += predicted * z\n",
    "        gradPred += outputVectors[samp] * z\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradPred, grad\n",
    "\n",
    "\n",
    "def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors,\n",
    "             dataset, word2vecCostAndGradient=softmaxCostAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec\n",
    "\n",
    "    Implement the skip-gram model in this function.\n",
    "\n",
    "    Arguments:\n",
    "    currrentWord -- a string of the current center word\n",
    "    C -- integer, context size\n",
    "    contextWords -- list of no more than 2*C strings, the context words\n",
    "    tokens -- a dictionary that maps words to their indices in\n",
    "              the word vector list\n",
    "    inputVectors -- \"input\" word vectors (as rows) for all tokens\n",
    "    outputVectors -- \"output\" word vectors (as rows) for all tokens\n",
    "    word2vecCostAndGradient -- the cost and gradient function for\n",
    "                               a prediction vector given the target\n",
    "                               word vectors, could be one of the two\n",
    "                               cost functions you implemented above.\n",
    "\n",
    "    Return:\n",
    "    cost -- the cost function value for the skip-gram model\n",
    "    grad -- the gradient with respect to the word vectors\n",
    "    \"\"\"\n",
    "\n",
    "    cost = 0.0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    cword_idx = tokens[currentWord]\n",
    "    vhat = inputVectors[cword_idx]\n",
    "\n",
    "    for j in contextWords:\n",
    "        u_idx = tokens[j]\n",
    "        c_cost, c_grad_in, c_grad_out = \\\n",
    "            word2vecCostAndGradient(vhat, u_idx, outputVectors, dataset)\n",
    "        cost += c_cost\n",
    "        gradIn[cword_idx] += c_grad_in\n",
    "        gradOut += c_grad_out\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradIn, gradOut\n",
    "\n",
    "\n",
    "def cbow(currentWord, C, contextWords, tokens, inputVectors, outputVectors,\n",
    "         dataset, word2vecCostAndGradient=softmaxCostAndGradient):\n",
    "    \"\"\"CBOW model in word2vec\n",
    "\n",
    "    Implement the continuous bag-of-words model in this function.\n",
    "\n",
    "    Arguments/Return specifications: same as the skip-gram model\n",
    "\n",
    "    Extra credit: Implementing CBOW is optional, but the gradient\n",
    "    derivations are not. If you decide not to implement CBOW, remove\n",
    "    the NotImplementedError.\n",
    "    \"\"\"\n",
    "\n",
    "    cost = 0.0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    predicted_indices = [tokens[word] for word in contextWords]\n",
    "    predicted_vectors = inputVectors[predicted_indices]\n",
    "    predicted = np.sum(predicted_vectors, axis=0)\n",
    "    target = tokens[currentWord]\n",
    "    cost, gradIn_predicted, gradOut = word2vecCostAndGradient(predicted, target, outputVectors, dataset)\n",
    "    for i in predicted_indices:\n",
    "        gradIn[i] += gradIn_predicted\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradIn, gradOut\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Testing functions below. DO NOT MODIFY!   #\n",
    "#############################################\n",
    "\n",
    "def word2vec_sgd_wrapper(word2vecModel, tokens, wordVectors, dataset, C,\n",
    "                         word2vecCostAndGradient=softmaxCostAndGradient):\n",
    "    batchsize = 50\n",
    "    cost = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    inputVectors = wordVectors[:N / 2, :]\n",
    "    outputVectors = wordVectors[N / 2:, :]\n",
    "    for i in xrange(batchsize):\n",
    "        C1 = random.randint(1, C)\n",
    "        centerword, context = dataset.getRandomContext(C1)\n",
    "\n",
    "        if word2vecModel == skipgram:\n",
    "            denom = 1\n",
    "        else:\n",
    "            denom = 1\n",
    "\n",
    "        c, gin, gout = word2vecModel(\n",
    "            centerword, C1, context, tokens, inputVectors, outputVectors,\n",
    "            dataset, word2vecCostAndGradient)\n",
    "        cost += c / batchsize / denom\n",
    "        grad[:N / 2, :] += gin / batchsize / denom\n",
    "        grad[N / 2:, :] += gout / batchsize / denom\n",
    "\n",
    "    return cost, grad\n",
    "\n",
    "\n",
    "def test_word2vec():\n",
    "    \"\"\" Interface to the dataset for negative sampling \"\"\"\n",
    "    dataset = type('dummy', (), {})()\n",
    "\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0, 4)], \\\n",
    "               [tokens[random.randint(0, 4)] for i in xrange(2 * C)]\n",
    "\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10, 3))\n",
    "    dummy_tokens = dict([(\"a\", 0), (\"b\", 1), (\"c\", 2), (\"d\", 3), (\"e\", 4)])\n",
    "    print \"==== Gradient check for skip-gram ====\"\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n",
    "                    dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient),\n",
    "                    dummy_vectors)\n",
    "    print \"\\n==== Gradient check for CBOW      ====\"\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        cbow, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n",
    "                    dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        cbow, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient),\n",
    "                    dummy_vectors)\n",
    "\n",
    "    print \"\\n=== Results ===\"\n",
    "    print skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
    "                   dummy_tokens, dummy_vectors[:5, :], dummy_vectors[5:, :], dataset)\n",
    "    print skipgram(\"c\", 1, [\"a\", \"b\"],\n",
    "                   dummy_tokens, dummy_vectors[:5, :], dummy_vectors[5:, :], dataset,\n",
    "                   negSamplingCostAndGradient)\n",
    "    print cbow(\"a\", 2, [\"a\", \"b\", \"c\", \"a\"],\n",
    "               dummy_tokens, dummy_vectors[:5, :], dummy_vectors[5:, :], dataset)\n",
    "    print cbow(\"a\", 2, [\"a\", \"b\", \"a\", \"c\"],\n",
    "               dummy_tokens, dummy_vectors[:5, :], dummy_vectors[5:, :], dataset,\n",
    "               negSamplingCostAndGradient)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_normalize_rows()\n",
    "    test_word2vec()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
