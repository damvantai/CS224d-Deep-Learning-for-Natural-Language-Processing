{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from utils.general_utils import test_all_close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \n",
    "    \"\"\"Args:\n",
    "        x: tf.Tensor with shape (n_samples, n_features). Note feature vectors are\n",
    "            feature vectors are\n",
    "            represented by row-vectors. (For simplicity, no need to handle 1-d)\n",
    "    Returns:\n",
    "        out: tf.Tensor with shape (n_sample, n_features).\n",
    "    \"\"\"\n",
    "    x_max = tf.reduce_max(x, 1, keep_dims=True)\n",
    "    \n",
    "    x_sub = tf.subtract(x, x_max)\n",
    "    \n",
    "    x_exp = tf.exp(x_sub)\n",
    "    \n",
    "    sum_exp = tf.reduce_sum(x_exp, 1, keep_dims=True)\n",
    "    out = tf.div(x_exp, sum_exp)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the cross entropy loss in tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y, yhat):\n",
    "    \"\"\"The loss should be summed over the current minibatch\n",
    "    \n",
    "    y is a one hot tensor of shape (n_samples, n_classes) and yhat is a tensor\n",
    "    \n",
    "    of shape (n_samples, n_classes). y should be of dtype tf.int32, and yhat should\n",
    "    tf.float32.\n",
    "    \n",
    "    Args:\n",
    "        y: tf.Tensor with shape(n_samples, n_classes). One-hot encoded.\n",
    "        yhat: tf.Tensor with shape(n_samples, n_classes). Each row encodes a\n",
    "                probability distribution and should sum to 1.\n",
    "        \n",
    "    Return:\n",
    "        out: tf.Tensor with shape (1, ) (Scalar output)\n",
    "    \"\"\"\n",
    "    l_yhat = tf.log(yhat)\n",
    "    product = tf.multiply(tf.to_float(y), l_yhat)\n",
    "    \n",
    "    out = tf.negative(tf.reduce_sum(product))\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some simple tests of softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.26894143  0.7310586 ]\n",
      " [ 0.26894143  0.7310586 ]]\n"
     ]
    }
   ],
   "source": [
    "test1 = softmax(tf.constant(np.array([[1001, 1002], [3, 4]]), dtype=tf.float32))\n",
    "with tf.Session() as sess:\n",
    "    test1 = sess.run(test1)\n",
    "print(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1001, 1002], [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1001, 1002],\n",
       "       [   3,    4]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests of cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([[0, 1], [1, 0], [1, 0]])\n",
    "yhat = np.array([[.5, .5], [.5, .5], [.5, .5]])\n",
    "\n",
    "test1 = cross_entropy_loss(\n",
    "        tf.constant(y, dtype=tf.int32),\n",
    "        tf.constant(yhat, dtype=tf.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    test1 = sess.run(test1)\n",
    "expected = -3 * np.log(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.07944 nan\n"
     ]
    }
   ],
   "source": [
    "print(test1, expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_weight_init():\n",
    "    \"\"\"\n",
    "    Returns function that creates random tensor.\n",
    "    \n",
    "    The specified function will take in a shape (tuple or 1-d array)\n",
    "    anh returns a random tensor of the specified shape drawn form the Xavier initilization distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    def _xavier_initializer(shape, **kwargs):\n",
    "        \"\"\"\n",
    "        Defines an initializer for the Xavier distribution.\n",
    "        Specifically, the output should be sampled uniformly from\n",
    "        [-epsilon, epsilon] where\n",
    "            epsilon = sqrt(6) / <sum of the sizes of shape's dimensions>\n",
    "            \n",
    "        This function will be used as a variable initializer.\n",
    "        \n",
    "        Returns:\n",
    "            out: tf.Tensor of specified shape sampled from the Xavier distribution.\n",
    "        \"\"\"\n",
    "        \n",
    "        epsilon = np.sqrt(6 / np.sum(shape))\n",
    "        out = tf.Variable(tf.random_uniform(shape=shape, minval=-epsilon, maxval=epsilon))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    return _xavier_initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test initialization basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "xavier_initializer = xavier_weight_init()\n",
    "shape = (1,)\n",
    "xavier_mat = xavier_initializer(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(1,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "print(xavier_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_1:0' shape=(1, 2, 3) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "shape = (1, 2, 3)\n",
    "xavier_mat = xavier_initializer(shape)\n",
    "print(xavier_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 partial parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialParse(object):\n",
    "    def __init__(self, sentence):\n",
    "        \"\"\"\n",
    "        Initializers this partial parse\n",
    "        \n",
    "        Your code should initialize the following fields:\n",
    "            self.stack: The current stack represented as a list with the top of the stack as the last element of the list\n",
    "            \n",
    "            self.buffer: The current buffer represented as a list with the first item on the the buffer as the first item of the list\n",
    "            \n",
    "            self.dependencies: The list of dependencies producted so far.\n",
    "            Represented as a list of tuples where each tuple is of the form \n",
    "            \n",
    "            \n",
    "        The root token should be represented with the string \"ROOT\" \n",
    "        \n",
    "        Args:\n",
    "            sentence: The sentence to be parsed as a list of words.\n",
    "                Your code should not modify the sentence.\n",
    "                \n",
    "        \"\"\"\n",
    "        self.sentence = sentence\n",
    "        \n",
    "        self.stack = ['ROOT']\n",
    "        self.buffer = sentence[:]\n",
    "        self.dependencies = []\n",
    "    \n",
    "    def parse_step(self, transition):\n",
    "        \"\"\"\n",
    "        Performs a single parse step by applying the given \n",
    "        transition to this partial parse\n",
    "        \n",
    "        Args:\n",
    "            transition: A string that equals \"S\", \"LA\", or \"RA\"\n",
    "            representing the shift, left-arc, and right-arc transitions. \n",
    "        \"\"\"\n",
    "        if transition == \"S\":\n",
    "            self.stack.append(self.buffer[0])\n",
    "            self.buffer.pop(0)\n",
    "        elif transition == \"LA\":\n",
    "            self.dependencies.append((self.stack[-1], self.stack[-2]))\n",
    "            self.stack.pop(-2)\n",
    "        else:\n",
    "            self.dependencies.append((self.stack[-2], self.stack[-1]))\n",
    "            self.stack.pop(-1)\n",
    "    \n",
    "    def parse(self, transitions):\n",
    "        \"\"\"Applies the provided transitions to this PartialParse\n",
    "        \n",
    "        Args:\n",
    "            transitions: The list of transitions in the order they should be applied \n",
    "        \n",
    "        Returns:\n",
    "            dependencies: The list of dependencies produced when parsing the sentence. Represented\n",
    "             as a list of tuples where each tuple is of the form\n",
    "        \"\"\"\n",
    "        for transition in transitions:\n",
    "            self.parse_step(transition)\n",
    "        return self.dependencies\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parses a list of sentences in minibatches using a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_parse(sentences, model, batch_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sentences: A list of sentences to be parsed (each sentence is a list of words)\n",
    "        \n",
    "        model: The model that makes parsing decisions. It is assumed to have a function\n",
    "            model.predict(partial_parses) that takes in a list of PartialParses as input and\n",
    "            and returns a list of transitions predicted for each parse. \n",
    "            That is, after calling transitions = model. predict(partial_parses)\n",
    "            transitions[i] will be the next transition to apply to partial_parse[i]\n",
    "            \n",
    "        batch_size: The number of PartialParses to include in each minibatch\n",
    "        \n",
    "    Returns:\n",
    "        dependencies: A list where each element is the dependencies list for a parsed sentence\n",
    "        Ordering should be the same as in sentences\n",
    "    \"\"\"\n",
    "    partial_parses = [PartialParse(s) for s in sentences]\n",
    "    \n",
    "    unfinished_parse = partial_parses\n",
    "    \n",
    "    while len(unfinished_parse) > 0:\n",
    "        minibatch = unfinished_parse[0:batch_size]\n",
    "        \n",
    "        while len(minibatch) > 0:\n",
    "            transitions = model.predict(minibatch)\n",
    "            for index, action in enumerate(transitions):\n",
    "                minibatch[index].parse_step(action)\n",
    "            minibatch = [parse for parse in minibatch if len(parse.stack) > 1 or len(parse.buffer) > 0]\n",
    "            \n",
    "        # move to the next batch\n",
    "        unfinished_parse = unfinished_parse[batch_size:]\n",
    "    \n",
    "    dependencies = []\n",
    "    for n in range(len(sentences)):\n",
    "        dependencies.append(partial_parses[n].dependencies)\n",
    "        \n",
    "    return dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests that a single parse step returns the expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(name, transition, stack, buf, deps, ex_stack, ex_buf, ex_deps):\n",
    "    pp = PartialParse([])\n",
    "    pp.stack, pp.buffer, pp.dependencies = stack, buf, deps\n",
    "    \n",
    "    pp.parse_step(transition)\n",
    "    \n",
    "    stack, buf, deps = (tuple(pp.stack), tuple(pp.buffer), tuple(sorted(pp.dependencies)))\n",
    "    print(stack)\n",
    "    print(buf)\n",
    "    print(deps)\n",
    "    print(stack==ex_stack, buf==ex_buf, deps==ex_deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ROOT', 'the', 'cat')\n",
      "('sat',)\n",
      "()\n",
      "True True True\n"
     ]
    }
   ],
   "source": [
    "test_step(\"SHIFT\", \"S\", [\"ROOT\", \"the\"], [\"cat\", \"sat\"], [], (\"ROOT\", \"the\", \"cat\"), (\"sat\",), ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ROOT', 'cat')\n",
      "('sat',)\n",
      "(('cat', 'the'),)\n",
      "True True True\n"
     ]
    }
   ],
   "source": [
    "test_step(\"LEFT-ARC\", \"LA\", [\"ROOT\", \"the\", \"cat\"], [\"sat\"], []\n",
    "         , (\"ROOT\", \"cat\",), (\"sat\",), ((\"cat\", \"the\"),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ROOT', 'run')\n",
      "()\n",
      "(('run', 'fast'),)\n",
      "True True True\n"
     ]
    }
   ],
   "source": [
    "test_step(\"RIGHT-ARC\", \"RA\", [\"ROOT\", \"run\", \"fast\"], [], [], \n",
    "         (\"ROOT\", \"run\",), (), ((\"run\", \"fast\"),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple tests for the PartialParse.parse function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"parse\", \"this\", \"sentence\"]\n",
    "dependencies = PartialParse(sentence).parse([\"S\", \"S\", \"S\", \"LA\", \"RA\", \"RA\"])\n",
    "dependencies = tuple(sorted(dependencies))\n",
    "expected = (('ROOT', 'parse'), ('parse', 'sentence'), ('sentence', 'this'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('ROOT', 'parse'), ('parse', 'sentence'), ('sentence', 'this'))\n",
      "(('ROOT', 'parse'), ('parse', 'sentence'), ('sentence', 'this'))\n",
      "('parse', 'this', 'sentence')\n"
     ]
    }
   ],
   "source": [
    "print(dependencies)\n",
    "print(expected)\n",
    "print(tuple(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy model for testing the minibatch parse function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel:\n",
    "    \"\"\"\n",
    "    First shifts everything onto the stack ad then does exclusively right arcs if the first\n",
    "    word of the sentence is \"right\", \"left\" if otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    def predict(self, partial_parses):\n",
    "        return [(\"RA\" if pp.stack[1] is \"right\" else \"LA\") if len(pp.buffer) == 0 else \"S\"\n",
    "               for pp in partial_parses]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [[\"right\", \"arcs\", \"only\"],\n",
    "            [\"right\", \"arcs\", \"only\", \"again\"],\n",
    "            [\"left\", \"arcs\", \"only\"],\n",
    "            [\"left\", \"arcs\", \"only\", \"again\"]]\n",
    "deps = minibatch_parse(sentences, DummyModel(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "deps = tuple(sorted(deps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('again', 'only'), ('again', 'arcs'), ('again', 'left'), ('again', 'ROOT')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('arcs', 'only'), ('right', 'arcs'), ('ROOT', 'right')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deps[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \"\"\"Abstracts a Tensorflow graph for a learning task.\n",
    "\n",
    "    We use various Model classes as usual abstractions to encapsulate tensorflow\n",
    "    computational graphs. Each algorithm you will construct in this homework will\n",
    "    inherit from a Model object.\n",
    "    \"\"\"\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Adds placeholder variables to tensorflow computational graph.\n",
    "\n",
    "        Tensorflow uses placeholder variables to represent locations in a\n",
    "        computational graph where data is inserted.  These placeholders are used as\n",
    "        inputs by the rest of the model building and will be fed data during\n",
    "        training.\n",
    "\n",
    "        See for more information:\n",
    "        https://www.tensorflow.org/versions/r0.7/api_docs/python/io_ops.html#placeholders\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
    "\n",
    "    def create_feed_dict(self, inputs_batch, labels_batch=None):\n",
    "        \"\"\"Creates the feed_dict for one step of training.\n",
    "\n",
    "        A feed_dict takes the form of:\n",
    "        feed_dict = {\n",
    "                <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "                ....\n",
    "        }\n",
    "\n",
    "        If labels_batch is None, then no labels are added to feed_dict.\n",
    "\n",
    "        Hint: The keys for the feed_dict should be a subset of the placeholder\n",
    "                    tensors created in add_placeholders.\n",
    "\n",
    "        Args:\n",
    "            inputs_batch: A batch of input data.\n",
    "            labels_batch: A batch of label data.\n",
    "        Returns:\n",
    "            feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
    "\n",
    "    def add_prediction_op(self):\n",
    "        \"\"\"Implements the core of the model that transforms a batch of input data into predictions.\n",
    "\n",
    "        Returns:\n",
    "            pred: A tensor of shape (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
    "\n",
    "    def add_loss_op(self, pred):\n",
    "        \"\"\"Adds Ops for the loss function to the computational graph.\n",
    "\n",
    "        Args:\n",
    "            pred: A tensor of shape (batch_size, n_classes)\n",
    "        Returns:\n",
    "            loss: A 0-d tensor (scalar) output\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
    "\n",
    "    def add_training_op(self, loss):\n",
    "        \"\"\"Sets up the training Ops.\n",
    "\n",
    "        Creates an optimizer and applies the gradients to all trainable variables.\n",
    "        The Op returned by this function is what must be passed to the\n",
    "        sess.run() to train the model. See\n",
    "\n",
    "        https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n",
    "\n",
    "        for more information.\n",
    "\n",
    "        Args:\n",
    "            loss: Loss tensor (a scalar).\n",
    "        Returns:\n",
    "            train_op: The Op for training.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
    "\n",
    "    def train_on_batch(self, sess, inputs_batch, labels_batch):\n",
    "        \"\"\"Perform one step of gradient descent on the provided batch of data.\n",
    "\n",
    "        Args:\n",
    "            sess: tf.Session()\n",
    "            input_batch: np.ndarray of shape (n_samples, n_features)\n",
    "            labels_batch: np.ndarray of shape (n_samples, n_classes)\n",
    "        Returns:\n",
    "            loss: loss over the batch (a scalar)\n",
    "        \"\"\"\n",
    "        feed = self.create_feed_dict(inputs_batch, labels_batch=labels_batch)\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict=feed)\n",
    "        return loss\n",
    "\n",
    "    def predict_on_batch(self, sess, inputs_batch):\n",
    "        \"\"\"Make predictions for the provided batch of data\n",
    "\n",
    "        Args:\n",
    "            sess: tf.Session()\n",
    "            input_batch: np.ndarray of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            predictions: np.ndarray of shape (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        feed = self.create_feed_dict(inputs_batch)\n",
    "        predictions = sess.run(self.pred, feed_dict=feed)\n",
    "        return predictions\n",
    "\n",
    "    def build(self):\n",
    "        self.add_placeholders()\n",
    "        self.pred = self.add_prediction_op()\n",
    "        self.loss = self.add_loss_op(self.pred)\n",
    "        self.train_op = self.add_training_op(self.loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
